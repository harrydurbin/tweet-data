{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modules and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import folium\n",
    "import gc \n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import MultiPoint, Polygon, Point, LineString\n",
    "from sklearn.cluster import MiniBatchKMeans, DBSCAN\n",
    "from sklearn import metrics\n",
    "import sqlalchemy\n",
    "# import geoalchemy2\n",
    "import psycopg2\n",
    "import os\n",
    "import subprocess\n",
    "from scipy.spatial import ConvexHull\n",
    "# from scipy.spatial import Delaunay\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tweets_1M.json\",'r') as f:\n",
    "    tweets=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>text</th>\n",
       "      <th>timeStamp</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>224874450</td>\n",
       "      <td>@Tanner_Cortez hey checkout the website: http:...</td>\n",
       "      <td>Wed Sep 11 04:38:08 +0000 2013</td>\n",
       "      <td>37.446100</td>\n",
       "      <td>-121.883557</td>\n",
       "      <td>377652254096228352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>312179473</td>\n",
       "      <td>@Lennayy7 ðŸ˜ª</td>\n",
       "      <td>Wed Sep 11 04:38:08 +0000 2013</td>\n",
       "      <td>34.087406</td>\n",
       "      <td>-117.462604</td>\n",
       "      <td>377652255346159616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54351774</td>\n",
       "      <td>i laugh a lot with that line</td>\n",
       "      <td>Wed Sep 11 04:38:10 +0000 2013</td>\n",
       "      <td>37.356131</td>\n",
       "      <td>-121.842867</td>\n",
       "      <td>377652262325456897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>343219606</td>\n",
       "      <td>sons of anarchy is back on woop woop</td>\n",
       "      <td>Wed Sep 11 04:38:11 +0000 2013</td>\n",
       "      <td>37.364664</td>\n",
       "      <td>-122.009629</td>\n",
       "      <td>377652264682655744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1569395935</td>\n",
       "      <td>Drinking a Fresh Squeezed IPA by @deschutesbee...</td>\n",
       "      <td>Wed Sep 11 04:38:12 +0000 2013</td>\n",
       "      <td>37.382600</td>\n",
       "      <td>-121.995000</td>\n",
       "      <td>377652271116722176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                                               text  \\\n",
       "0   224874450  @Tanner_Cortez hey checkout the website: http:...   \n",
       "1   312179473                                        @Lennayy7 ðŸ˜ª   \n",
       "2    54351774                       i laugh a lot with that line   \n",
       "3   343219606               sons of anarchy is back on woop woop   \n",
       "4  1569395935  Drinking a Fresh Squeezed IPA by @deschutesbee...   \n",
       "\n",
       "                        timeStamp        lat         lng                  id  \n",
       "0  Wed Sep 11 04:38:08 +0000 2013  37.446100 -121.883557  377652254096228352  \n",
       "1  Wed Sep 11 04:38:08 +0000 2013  34.087406 -117.462604  377652255346159616  \n",
       "2  Wed Sep 11 04:38:10 +0000 2013  37.356131 -121.842867  377652262325456897  \n",
       "3  Wed Sep 11 04:38:11 +0000 2013  37.364664 -122.009629  377652264682655744  \n",
       "4  Wed Sep 11 04:38:12 +0000 2013  37.382600 -121.995000  377652271116722176  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to state plane coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lat_conv'] = (df.lat - 32)*89700 # convert to meters with origin at 32d N (1d lat = 89.7km)\n",
    "df['lng_conv'] = (df.lng + 126)*112700 # convert to meters with origin at 126d W (1d lng = 112.7km)\n",
    "df = df.assign(crds=df[['lat_conv', 'lng_conv']].values.tolist())\n",
    "# # define the number of kilometers in one radian\n",
    "# kms_per_radian = 6371.0088\n",
    "# # define epsilon as 1.5 kilometers, converted to radians for use by haversine\n",
    "# EPSILON = 1.5 / kms_per_radian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Mini-Batch KMeans to divide data into spatial clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100 # set the number of clusters to split the data into\n",
    "def runMBKmeans(df):\n",
    "    # RUNNING MINIBATCH KMEANS ON ENTIRE SET \n",
    "#     print ('Beginning to run MiniBatch Kmeans.')\n",
    "    ## initialize with MBK-means++, a good way to speed up convergence more than K-Means\n",
    "    mbk_means = MiniBatchKMeans(init='k-means++', n_clusters=n, n_init=10, init_size = 3*n, verbose = 0) # set minibatch kmeans parameters\n",
    "    mbk_means.fit(list(df.crds.values)) # run mini batch k means on the lat and lng coordinate data\n",
    "    df['mbk_means_labels'] = mbk_means.labels_ # this is an array that indicates which cluster each tweet is in\n",
    "    df_output = df\n",
    "    mbk_means_cluster_centers = mbk_means.cluster_centers_ # this is an array that indicates which cluster each tweet is in\n",
    "    df0 = pd.DataFrame()\n",
    "    df0= pd.DataFrame(mbk_means_cluster_centers,columns = ['lat','lng'])\n",
    "    df0 = pd.DataFrame(mbk_means_cluster_centers,columns = ['lat_centroid','lng_centroid'])\n",
    "    df0['unique_tweeters'] = df.groupby('mbk_means_labels')['user_id'].nunique()\n",
    "    df0['unique_tweets'] = df.groupby('mbk_means_labels')['mbk_means_labels'].count()\n",
    "#     print ('Finished running Kmeans.')\n",
    "    return df_output, df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df,df0 = runMBKmeans(df)\n",
    "df0['labels'] = np.arange(n)\n",
    "df['db_labels'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running DBSCAN on each cluster generated by MiniBatch Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runDBSCAN(df,EPSILON):\n",
    "    coords = list(df.crds.values)\n",
    "    db = DBSCAN(eps=EPSILON).fit(coords)\n",
    "    db_labels = db.labels_ # create DBSCAN cluster labels generated for the cluster generated by MBKmeans\n",
    "    db_labels_unique = np.unique(db_labels)\n",
    "    k_DB = len(db_labels_unique)-1   #number of clusters detected with DBSCAN, subtract one because one cluster is noise (-1)    \n",
    "    DB_noise = list(db_labels).count(-1) # these are tweets that DBScan determined to be noise\n",
    "    df.db_labels = list(db.labels_)\n",
    "#     print ('Number of sub-clusters detected is: ', k_DB)\n",
    "#     print ('Noise tweet count is: ', DB_noise)\n",
    "#     print ('Important (non-noise) tweet count is : ', len(df)-DB_noise)\n",
    "    collected = gc.collect() \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster # 0 Run Time:  3.0 seconds\n",
      "Cluster # 1 Run Time:  1.0 seconds\n",
      "Cluster # 2 Run Time:  2.0 seconds\n",
      "Cluster # 3 Run Time:  1.0 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3aaab3862b0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_cluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mdf_cluster1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunDBSCAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_cluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mbk_means_labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_cluster1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3462\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3463\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3464\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3466\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_setitem_array\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3481\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3482\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_setitem_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3483\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3484\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3485\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    498\u001b[0m                             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m                         \u001b[0msetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m                 \u001b[0;31m# we have an equal len ndarray/convertible to our labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36msetter\u001b[0;34m(item, v)\u001b[0m\n\u001b[1;32m    473\u001b[0m                     \u001b[0;31m# set the item, possibly having a dtype change\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m                     \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                     \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                     \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m   5995\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5996\u001b[0m         \"\"\"\n\u001b[0;32m-> 5997\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5998\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    785\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0mnew_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"copy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_integrity_check\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mas_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malign_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_block_same_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clusters = np.arange(n)\n",
    "EPSILON = 1600*2 #meters\n",
    "for cluster in clusters:\n",
    "#     print ('________________________________________')\n",
    "    start_time = time.time()\n",
    "    df_cluster = df[df['mbk_means_labels']==cluster]\n",
    "    if len(df_cluster)>1:\n",
    "        df_cluster1 = runDBSCAN(df_cluster,EPSILON)\n",
    "        df[df['mbk_means_labels']==cluster] = df_cluster1\n",
    "        elapsed_time = time.time() - start_time\n",
    "    else:\n",
    "        next\n",
    "    print('Cluster #', cluster, 'Run Time: ', round(elapsed_time,0), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing points determined to be noise in DBSCAN (i.e. cluster labels of '-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.db_labels!= -1]\n",
    "print ('Rows of data reduced from 1000000 to' , str(len(df)) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a unique cluster label that combines both kmeans and dbscan cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'] = (df.mbk_means_labels).astype(str) + '.' + (df.db_labels).astype(str)\n",
    "num_of_clusters = len(df.groupby('labels').labels.nunique())\n",
    "print (num_of_clusters , \"clusters were found in the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write data to  psql / postgis database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = 'harry'\n",
    "PASSWORD = 'harry'\n",
    "enginestring = 'postgresql://{}:{}@localhost:5433/tweets'.format(USER, PASSWORD)\n",
    "engine = sqlalchemy.create_engine(enginestring)\n",
    "\n",
    "conn = psycopg2.connect(\"dbname=tweets user=harry host=localhost port=5433 password='harry'\")\n",
    "cur = conn.cursor()\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql('tweetpoints', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT AddGeometryColumn ('tweetpoints', 'location', 4326, 'POINT', 2);\")\n",
    "cur.execute(\"UPDATE tweetpoints SET location = ST_SetSRID (ST_Point(lng, lat), 4326);\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new dataframe to hold cluster information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame()\n",
    "df1['label'] = df.groupby('labels').labels.nunique().index.values\n",
    "df1['tweetcount'] =  df.groupby('labels').user_id.count().values\n",
    "df1['usercount'] =  df.groupby('labels').user_id.nunique().values\n",
    "df1 = df1.sort_values('tweetcount', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out the clusters that contain less than a thousand tweets in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 250\n",
    "df2 = df1[df1.tweetcount >= threshold]\n",
    "df2 = df2[df2.usercount > 1] # remove clusters that consist of only one user account\n",
    "print (len(df2), \"clusters contain at least\", threshold,\"tweets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate distribution of the number of tweets in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "for col in ['tweetcount']: #,'usercount']:\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    sns.distplot(df2[col], kde=False, bins=10)\n",
    "    plt.title('Density Distribution', fontsize=18)\n",
    "    plt.xlabel('Count of Tweets per Cluster', fontsize=16)\n",
    "    plt.ylabel('Frequency', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through labels to determin the convex hull (approximate perimeter) around each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for kmeans clusters\n",
    "df0['geom'] = \"\"\n",
    "for cluster in df0.labels:\n",
    "    df3 = df[df.mbk_means_labels == cluster]\n",
    "    points = [[float(lng), float(lat)] for lng, lat in zip(df3.lng, df3.lat)]\n",
    "    multi_point = MultiPoint(points)\n",
    "    convex = multi_point.convex_hull\n",
    "    hull = str(convex)\n",
    "    df0.loc[df0['labels'] == cluster, ['geom']] = hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dbscan clusters\n",
    "df2['geom'] = \"\"\n",
    "for cluster in df2.label:\n",
    "    df3 = df[df.labels == cluster]\n",
    "    points = [[float(lng), float(lat)] for lng, lat in zip(df3.lng, df3.lat)]\n",
    "    multi_point = MultiPoint(points)\n",
    "    convex = multi_point.convex_hull\n",
    "    hull = str(convex)\n",
    "    df2.loc[df2['label'] == cluster, ['geom']] = hull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the cluster dataframe to a psql/postgis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.to_sql('mbkmclusters', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_sql('clusters', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alter the psql/postgis clusters table to have spatial geometry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT AddGeometryColumn ('mbkmclusters', 'geometry', 4326, 'POLYGON', 2);\")\n",
    "cur.execute(\"UPDATE mbkmclusters SET geometry = ST_SetSRID(ST_GeomFromText(geom), 4326);\")\n",
    "cur.execute(\"ALTER TABLE mbkmclusters DROP COLUMN geom;\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT AddGeometryColumn ('clusters', 'geometry', 4326, 'POLYGON', 2);\")\n",
    "cur.execute(\"UPDATE clusters SET geometry = ST_SetSRID(ST_GeomFromText(geom), 4326);\")\n",
    "cur.execute(\"ALTER TABLE clusters DROP COLUMN geom;\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.rollback() # use this command if error is given during query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load shapefile of counties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) copy shapefile of counties\n",
    "\n",
    "2) change directory to location of shapepile\n",
    "\n",
    "3) in terminal: sudo -su postgres\n",
    "\n",
    "4) in terminal: shp2pgsql -I -W \"latin1\" -s 4326 tl_2010_06_county10/tl_2010_06_county10.shp tl_2010_06_county10 | psql -d tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"Select * FROM tl_2010_06_county10;\")\n",
    "colnames = [desc[0] for desc in cur.description]\n",
    "print (colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"Select * FROM ca_census_data;\")\n",
    "colnames = [desc[0] for desc in cur.description]\n",
    "print (colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"Select * FROM clusters;\")\n",
    "colnames = [desc[0] for desc in cur.description]\n",
    "print (colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy census population data per county into posgis table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_census_data = pd.read_csv('all_050_in_06.P1.csv')\n",
    "colnames = [x.lower() for x in ca_census_data.columns.values] # convert column names to lowercase\n",
    "ca_census_data.columns = colnames\n",
    "ca_census_data.to_sql('ca_census_data', engine, if_exists='replace')\n",
    "ca_census_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population per county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT name as county, pop100 AS population\n",
    "FROM ca_census_data\n",
    "ORDER BY pop100;'''\n",
    "\n",
    "x = pd.read_sql_query(sql, con=conn)\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of tweets per county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT t1.namelsad10 AS county, COUNT(T2.id) AS tweetcount\n",
    "FROM tl_2010_06_county10 T1,tweetpoints T2\n",
    "WHERE ST_Contains(T1.geom,T2.location)\n",
    "GROUP BY T1.namelsad10 \n",
    "ORDER BY T1.namelsad10;\n",
    "\"\"\"\n",
    "\n",
    "y = pd.read_sql_query(sql, con=conn)\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets per capita in each county"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create postgis table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "DROP TABLE IF EXISTS tweetspercapita;\n",
    "CREATE TABLE tweetspercapita AS\n",
    "(SELECT t3.namelsad10 AS county, t3.x AS tweetcount, t4.pop100 AS population, ROUND(t3.x*100.0/t4.pop100,2) AS tweetspercapita, t5.geom\n",
    "FROM\n",
    "    (SELECT t1.namelsad10, CAST(COUNT(t2.id) AS NUMERIC) AS x\n",
    "    FROM tl_2010_06_county10 t1,tweetpoints t2\n",
    "    WHERE ST_Contains(t1.geom,t2.location)\n",
    "    GROUP BY t1.namelsad10 \n",
    "    ORDER BY t1.namelsad10) AS t3\n",
    "    LEFT JOIN\n",
    "        (SELECT name, pop100\n",
    "        FROM ca_census_data\n",
    "        ORDER BY pop100) AS t4\n",
    "    ON t3.namelsad10 = t4.name\n",
    "    LEFT JOIN\n",
    "        (SELECT geom, namelsad10\n",
    "        FROM tl_2010_06_county10) AS t5\n",
    "    ON t4.name = t5.namelsad10);\"\"\"\n",
    "cur.execute(sql)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create dataframe from query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT t3.namelsad10 AS county, t3.x AS tweetcount, t4.pop100 AS population, ROUND(t3.x*100.0/t4.pop100,2) AS tweetspercapita, t5.geom\n",
    "FROM\n",
    "    (SELECT t1.namelsad10, CAST(COUNT(t2.id) AS NUMERIC) AS x\n",
    "    FROM tl_2010_06_county10 t1,tweetpoints t2\n",
    "    WHERE ST_Contains(t1.geom,t2.location)\n",
    "    GROUP BY t1.namelsad10 \n",
    "    ORDER BY t1.namelsad10) AS t3\n",
    "    LEFT JOIN\n",
    "        (SELECT name, pop100\n",
    "        FROM ca_census_data\n",
    "        ORDER BY pop100) AS t4\n",
    "    ON t3.namelsad10 = t4.name\n",
    "    LEFT JOIN\n",
    "        (SELECT geom, namelsad10\n",
    "        FROM tl_2010_06_county10) AS t5\n",
    "    ON t4.name = t5.namelsad10;\"\"\"\n",
    "\n",
    "df4 = pd.read_sql_query(sql, con=conn)\n",
    "df4.sort_values('tweetspercapita',ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write the dataframe to a geojson file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_counties = gpd.read_file(\"tl_2010_06_county10/tl_2010_06_county10.shp\")\n",
    "geometry = gdf_counties.geometry\n",
    "gdf = gpd.GeoDataFrame(df4, geometry=geometry)\n",
    "gdf.to_file('tweetspercapita.geojson', driver='GeoJSON')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
